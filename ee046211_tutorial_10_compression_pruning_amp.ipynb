{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <img src=\"./assets/course-icon.png\" style=\"height:50px;display:inline\"> EE 046211 - Technion - Deep Learning\n",
    "---\n",
    "\n",
    "#### <a href=\"https://taldatech.github.io\">Tal Daniel</a>\n",
    "\n",
    "## Tutorial 10 - Resource Efficiency - AMP, Quantization and Pruning\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### <img src=\"https://img.icons8.com/bubbles/50/000000/checklist.png\" style=\"height:50px;display:inline\"> Agenda\n",
    "---\n",
    "* [Resource Efficiency Motivation](#-Resource-Efficiency-Motivation)\n",
    "    * [Energy Required for Mathematical Operations](#-Energy-Required-for-Mathematical-Operations)\n",
    "* [Automatic Mixed Precision (AMP)](#-Train-time-Efficiency---Automatic-Mixed-Precision-(AMP))\n",
    "    * [AMP in PyTorch](#-AMP-in-PyTorch)\n",
    "* [Quantization](#-Inference/Train-time-Efficiency---Quantization)\n",
    "    * [Quantization in PyTorch](#-Quantization-in-PyTorch)\n",
    "    * [Which Quantization Method to Use?](#-Which-Quantization-Method-to-Use?)\n",
    "* [Pruning](#-Post-Train-time-Efficiency---Pruning)\n",
    "    * [The Lottery Ticket Hypothesis](#-The-Lottery-Ticket-Hypothesis)\n",
    "    * [Pruning in PyTorch](#-Pruning-in-PyTorch)\n",
    "* [Recommended Videos](#-Recommended-Videos)\n",
    "* [Credits](#-Credits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# imports for the tutorial\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "import gc\n",
    "\n",
    "# pytorch imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.quantization\n",
    "import torch.nn.utils.prune as prune"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## <img src=\"https://img.icons8.com/color/96/000000/winrar.png\" style=\"height:50px;display:inline\"> Resource Efficiency Motivation\n",
    "---\n",
    "* Training (large) neural networks is usually a long process that requires sufficient hardware.\n",
    "* Hardware, such as GPUs, consumes a lot of energy which, for environmental reasons, can be spared.\n",
    "* For real life applications, we ideally want fast inference times and we also want to deploy our models to, possibly, various devices such as mobile phones that don't have a matching hardware to the machine our models were trained on.\n",
    "* And of course, these things COST MONEY, and usually lots of it.\n",
    "* In this tutorial, we will discuss several approaches to incorporate compression during training and during inference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\"./assets/tut_compress_chart.PNG\" style=\"height:400px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### <img src=\"https://img.icons8.com/nolan/64/math.png\" style=\"height:50px;display:inline\"> Energy Required for Mathematical Operations\n",
    "---\n",
    "|Operation| MUL| ADD|\n",
    "|:---------|:----|:----|\n",
    "|8-bit Integer| 0.2 pJ|0.03 pJ|\n",
    "|32-bit Integer|3.1 pJ|0.1 pJ|\n",
    "|16-bit Floating Point| 1.1 pJ| 0.4 pJ|\n",
    "|32-bit Floating Point| 3.7 pJ| 0.9 pJ|\n",
    "\n",
    "How can we utilize this information to make training and inference more efficient?\n",
    "\n",
    "* pJ - pico-Joules $= 10^{-12}$ Joules, 1 Watt = 1 Joule per second. \n",
    "* <a href=\"https://ieeexplore.ieee.org/document/6757323\">Source</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## <img src=\"https://img.icons8.com/office/80/000000/mixer.png\" style=\"height:50px;display:inline\"> Train-time Efficiency - Automatic Mixed Precision (AMP)\n",
    "---\n",
    "* Deep Neural Network training has traditionally relied on FP32 (32-bit Floating Point, IEEE single-precision format).\n",
    "* The (automatic) mixed precision technique - training with FP16 (16-bit Floating Point, half-precision) while maintaining the network accuracy achieved with FP32.\n",
    "* Enabling mixed precision involves two steps: \n",
    "    * Porting the model to use the half-precision data type **where appropriate**.\n",
    "    * Using loss scaling to preserve small gradient values.\n",
    "\n",
    "<img src=\"./assets/tut_compress_amp.png\" style=\"height:300px\">\n",
    "\n",
    "<a href=\"https://developer.nvidia.com/automatic-mixed-precision\">Image Source</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### The Benefits of AMP\n",
    "---\n",
    "* Speeds up math-intensive operations, such as linear and convolution layers.\n",
    "* Speeds up memory-limited operations by accessing half the bytes compared to single-precision.\n",
    "* Reduces memory requirements for training models, enabling larger models or larger minibatches.\n",
    "* This feature enables automatic conversion of certain GPU operations from FP32 precision to mixed precision, thus improving performance while maintaining accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Performance of mixed precision training on NVIDIA 8xV100 vs. FP32 training on 8xV100 GPU**\n",
    "\n",
    "<img src=\"./assets/tut_compress_amp_chart.png\" style=\"height:300px\">\n",
    "\n",
    "* Bars represent the speedup factor of V100 AMP over V100 FP32. The higher the better.\n",
    "\n",
    "<a href=\"https://pytorch.org/blog/accelerating-training-on-nvidia-gpus-with-pytorch-automatic-mixed-precision/\">Image Source</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### <img src=\"https://img.icons8.com/cotton/64/000000/olympic-torch.png\" style=\"height:50px;display:inline\"> AMP in PyTorch\n",
    "---\n",
    "* We will follow an <a href=\"https://pytorch.org/tutorials/recipes/recipes/amp_recipe.html\">example</a> by <a href=\"https://github.com/mcarilli\">Michael Carilli</a>.\n",
    "* Note: this is only relevant for **GPU-powered machines**.\n",
    "    * Mixed precision primarily benefits Tensor Core-enabled architectures (Volta, Turing, Ampere), where one can obsereve significant (2-3X) speedup on those architectures. On earlier architectures (Kepler, Maxwell, Pascal), you may observe a modest speedup.\n",
    "* `torch.cuda.amp` provides convenience methods for mixed precision, where some operations use the `torch.float32` (float) datatype and other operations use `torch.float16` (half).\n",
    "* Some ops, like linear layers and convolutions, are much faster in `float16`, where other ops, like reductions, often require the dynamic range of `float32`.\n",
    "* Mixed precision tries to match each op to its appropriate datatype, which can reduce your network’s runtime and memory footprint.\n",
    "* AMP - “automatic mixed precision training” uses `torch.cuda.amp.autocast` and `torch.cuda.amp.GradScaler` together, as we will soon explain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# Timing utilities\n",
    "start_time = None\n",
    "\n",
    "def start_timer():\n",
    "    global start_time\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.reset_max_memory_allocated()\n",
    "    torch.cuda.synchronize()\n",
    "    start_time = time.time()\n",
    "\n",
    "def end_timer_and_print(local_msg):\n",
    "    torch.cuda.synchronize()\n",
    "    end_time = time.time()\n",
    "    print(\"\\n\" + local_msg)\n",
    "    print(\"Total execution time = {:.3f} sec\".format(end_time - start_time))\n",
    "    print(\"Max memory used by tensors = {} bytes\".format(torch.cuda.max_memory_allocated()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# for our demonstrations, we will use a simple MLP network\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "def make_model(in_size, out_size, num_layers):\n",
    "    layers = []\n",
    "    for _ in range(num_layers - 1):\n",
    "        layers.append(torch.nn.Linear(in_size, in_size))\n",
    "        layers.append(torch.nn.ReLU())\n",
    "    layers.append(torch.nn.Linear(in_size, out_size))\n",
    "    return torch.nn.Sequential(*tuple(layers)).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* `batch_size`, `in_size`, `out_size`, and `num_layers` are chosen to be large enough to saturate the GPU with work. \n",
    "* Typically, mixed precision provides the greatest speedup when the **GPU is saturated**. \n",
    "* Small networks may be CPU bound, in which case mixed precision won’t improve performance. \n",
    "* Sizes are also chosen such that the participating dimensions of the linear layers are multiples of 8, to permit Tensor Core usage on Tensor Core-capable GPUs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "batch_size = 512 # try, for example, 128, 256, 513.\n",
    "in_size = 4096\n",
    "out_size = 4096\n",
    "num_layers = 3\n",
    "num_batches = 50\n",
    "epochs = 3\n",
    "\n",
    "# creates data in default precision (fp32).\n",
    "# the same data is used for both default and mixed precision trials below.\n",
    "# you don't need to manually change inputs' dtype when enabling mixed precision.\n",
    "data = [torch.randn(batch_size, in_size, device=device) for _ in range(num_batches)]\n",
    "targets = [torch.randn(batch_size, out_size, device=device) for _ in range(num_batches)]\n",
    "\n",
    "loss_fn = torch.nn.MSELoss().to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Default Precision (FP32)\n",
    "---\n",
    "Without `torch.cuda.amp`, the following simple network executes all ops in default precision (`torch.float32`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\envs\\deep_learn\\lib\\site-packages\\torch\\cuda\\memory.py:263: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Default precision:\n",
      "Total execution time = 13.958 sec\n",
      "Max memory used by tensors = 1493322752 bytes\n"
     ]
    }
   ],
   "source": [
    "net = make_model(in_size, out_size, num_layers)\n",
    "opt = torch.optim.SGD(net.parameters(), lr=0.001)\n",
    "\n",
    "start_timer()\n",
    "for epoch in range(epochs):\n",
    "    for input, target in zip(data, targets):\n",
    "        output = net(input)\n",
    "        loss = loss_fn(output, target)\n",
    "        opt.zero_grad() # set_to_none=True here can modestly improve performance\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        \n",
    "end_timer_and_print(\"Default precision:\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Adding Autocast\n",
    "---\n",
    "* Instances of `torch.cuda.amp.autocast` serve as context managers that allow regions of your script to run in mixed precision.\n",
    "* In these regions, CUDA ops run in a `dtype` **chosen by `autocast`** to improve performance while maintaining accuracy. \n",
    "* See the <a href=\"https://pytorch.org/docs/stable/amp.html#autocast-op-reference\">Autocast Op Reference</a> for details on what precision autocast chooses for each op, and under what circumstances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "for epoch in range(0): # 0 epochs, this section is for illustration only\n",
    "    for input, target in zip(data, targets):\n",
    "        # Runs the forward pass under autocast.\n",
    "        with torch.cuda.amp.autocast():\n",
    "            output = net(input)\n",
    "            # output is float16 because linear layers autocast to float16.\n",
    "            assert output.dtype is torch.float16\n",
    "\n",
    "            loss = loss_fn(output, target)\n",
    "            # loss is float32 because mse_loss layers autocast to float32.\n",
    "            assert loss.dtype is torch.float32\n",
    "\n",
    "        # exits autocast before backward().\n",
    "        # backward passes under autocast are not recommended.\n",
    "        # backward ops run in the same dtype autocast chose for corresponding forward ops.\n",
    "        opt.zero_grad() # set_to_none=True here can modestly improve performance\n",
    "        loss.backward()\n",
    "        opt.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Adding GradScaler\n",
    "---\n",
    "* Gradient scaling helps **prevent gradients with small magnitudes from flushing to zero** (“underflowing”) when training with mixed precision.\n",
    "* `torch.cuda.amp.GradScaler` performs the steps of gradient scaling conveniently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# constructs scaler once, at the beginning of the convergence run, using default args.\n",
    "# the same GradScaler instance should be used for the entire convergence run.\n",
    "# if you perform multiple convergence runs in the same script, each run should use\n",
    "# a dedicated fresh GradScaler instance. GradScaler instances are lightweight.\n",
    "\n",
    "scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "for epoch in range(0): # 0 epochs, this section is for illustration only\n",
    "    for input, target in zip(data, targets):\n",
    "        # use autocast as before\n",
    "        with torch.cuda.amp.autocast():\n",
    "            output = net(input)\n",
    "            loss = loss_fn(output, target)\n",
    "\n",
    "        opt.zero_grad() # set_to_none=True here can modestly improve performance\n",
    "        \n",
    "        # scales loss. calls backward() on scaled loss to create scaled gradients.\n",
    "        scaler.scale(loss).backward()\n",
    "\n",
    "        # scaler.step() first unscales the gradients of the optimizer's assigned params.\n",
    "        # if these gradients do not contain infs or NaNs, optimizer.step() is then called,\n",
    "        # otherwise, optimizer.step() is skipped (!).\n",
    "        scaler.step(opt)\n",
    "\n",
    "        # Updates the scale for next iteration.\n",
    "        scaler.update()     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Putting it All Together: “Automatic Mixed Precision”\n",
    "---\n",
    "* The following also demonstrates `enabled`, an optional convenience argument to `autocast` and `GradScaler`. \n",
    "* If `False`, `autocast` and `GradScaler`’s calls become no-ops, which allows switching between default precision and mixed precision without if/else statements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\envs\\deep_learn\\lib\\site-packages\\torch\\cuda\\memory.py:263: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Mixed precision:\n",
      "Total execution time = 14.081 sec\n",
      "Max memory used by tensors = 1585620992 bytes\n"
     ]
    }
   ],
   "source": [
    "use_amp = True\n",
    "\n",
    "net = make_model(in_size, out_size, num_layers)\n",
    "opt = torch.optim.SGD(net.parameters(), lr=0.001)\n",
    "scaler = torch.cuda.amp.GradScaler(enabled=use_amp) # notice the `enabled` parameter\n",
    "\n",
    "start_timer()\n",
    "for epoch in range(epochs):\n",
    "    for input, target in zip(data, targets):\n",
    "        # notice the `enabled` parameter\n",
    "        with torch.cuda.amp.autocast(enabled=use_amp):\n",
    "            output = net(input)\n",
    "            loss = loss_fn(output, target)\n",
    "            \n",
    "        # set_to_none=True here can modestly improve performance, replace 0 with None (save mem)\n",
    "        opt.zero_grad(set_to_none=True) \n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(opt)\n",
    "        scaler.update()\n",
    "        \n",
    "end_timer_and_print(\"Mixed precision:\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* Here we got similar performance due to the GPU architecture (which is older).\n",
    "* For more advanced topics on AMP, <a href=\"https://pytorch.org/tutorials/recipes/recipes/amp_recipe.html\">see here</a>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## <img src=\"https://img.icons8.com/officel/80/000000/stairs.png\" style=\"height:50px;display:inline\"> Inference/Train-time Efficiency - Quantization\n",
    "---\n",
    "* Quantization refers to techniques for doing both computations and memory accesses with lower precision data, usually `int8` compared to floating point implementations. \n",
    "* Quantization leverages 8bit integer (`int8`) instructions to reduce the model size and run the inference faster (reduced latency).\n",
    "* This enables providing quick inference from a trained model and even fitting it into the resources available on a mobile device.\n",
    "* Quantization allows for siginificant performance gains!\n",
    "    * Up to 4x reduction in model size.\n",
    "    * Up to 2-4x reduction in memory bandwidth.\n",
    "    * Up to 2-4x faster inference due to savings in memory bandwidth and faster compute with int8 arithmetic (the exact speed up varies depending on the hardware, the runtime, and the model).\n",
    "* Quantization doesn't come without additional cost, as it means introducing approximations and the resulting networks have slightly less accuracy. \n",
    "* These techniques attempt to **minimize the gap between the full floating point accuracy and the quantized accuracy**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\"./assets/tut_compress_quant.png\" style=\"height:250px\">\n",
    "\n",
    "<a href=\"https://towardsdatascience.com/how-to-accelerate-and-compress-neural-networks-with-quantization-edfbbabb6af7\">Image Source</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### <img src=\"https://img.icons8.com/cotton/64/000000/olympic-torch.png\" style=\"height:50px;display:inline\"> Quantization in PyTorch\n",
    "---\n",
    "* Quantization is available in PyTorch in various flavors starting in version 1.3 and there are published quantized models for ResNet, ResNext, MobileNetV2, GoogleNet, InceptionV3 and ShuffleNetV2 in the PyTorch torchvision 0.5 library.\n",
    "* PyTorch has data types corresponding to quantized tensors, which share many of the features of tensors.\n",
    "* PyTorch supports quantized modules for common operations as part of the `torch.nn.quantized` and `torch.nn.quantized.dynamic` name-space.\n",
    "* Quantization is compatible with the rest of PyTorch: quantized models are traceable and scriptable. Quantized and floating point operations can be mixed in a model.\n",
    "* Mapping of floating point tensors to quantized tensors is customizable with user defined observer/fake-quantization blocks. PyTorch provides default implementations that should work for most use cases.\n",
    "* Currently the quantized models **can only be run on CPU**. However, it is possible to send the non-quantized parts of the model to a GPU.\n",
    "    * GPU quantization is a work-in-progress, see <a href=\"https://nvidia.github.io/TRTorch/tutorials/ptq.html\">PTQ (Post Training Quantization)</a>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\"./assets/tut_compress_quant_torch.png\" style=\"height:350px\">\n",
    "\n",
    "<a href=\"https://pytorch.org/blog/introduction-to-quantization-on-pytorch/\">Image Source</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### <img src=\"https://img.icons8.com/doodle/96/000000/three-fingers--v1.png\" style=\"height:50px;display:inline\"> The Three Types of Quantization\n",
    "---\n",
    "\n",
    "#### Dynamic Quantization\n",
    "---\n",
    "* Involves not just converting the weights to `int8` (as in all quantization variants), but also converting the **activations** to `int8` *on the fly*, just before doing the computation (hence “dynamic”).\n",
    "* The computations will be performed using efficient `int8` matrix multiplication and convolution implementations, resulting in faster compute. \n",
    "    * However, the activations are read and written to memory in floating point format.\n",
    "* In PyTorch: `torch.quantization.quantize_dynamic` - takes in a **model**, as well as a couple other arguments, and produces a quantized model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# dynamic quantization usage example\n",
    "quantized_model = torch.quantization.quantize_dynamic(model, {torch.nn.Linear}, dtype=torch.qint8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* Dynamic Quantization <a href=\"https://pytorch.org/docs/stable/quantization.html#torch.quantization.quantize_dynamic\">documentation</a>.\n",
    "* Examples (project ideas!): <a href=\"https://pytorch.org/tutorials/advanced/dynamic_quantization_tutorial.html\">LSTM word model quantization</a>, <a href=\"https://pytorch.org/tutorials/intermediate/dynamic_quantization_bert_tutorial.html\">pre-trained BERT quantization</a>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Post-Training Static Quantization\n",
    "---\n",
    "* Converting networks to use both integer arithmetic and `int8` memory accesses can improve the latency performance.\n",
    "* Static quantization first feeds batches of data through the network and computes the resulting distributions of the different activations.\n",
    "    * This is done by inserting “observer” modules at different points that record these distributions.\n",
    "* This information is used to determine how specifically the different activations should be quantized at **inference time**.\n",
    "    * A simple technique would be to simply divide the entire range of activations into 256 levels, but PyTorch supports more sophisticated methods as well.\n",
    "* This step allows to pass quantized values between operations instead of converting these values to floats - and then back to ints - between every operation, resulting in a significant speed-up.\n",
    "* Optimizing static quantization includes:\n",
    "    * **Observers**: observer modules specify how statistics are collected prior to quantization to try out more advanced methods to quantize the data.\n",
    "    * **Operator fusion**: fuse multiple operations into a single operation, saving on memory access while also improving the operation’s numerical accuracy.\n",
    "    * **Per-channel quantization**: we can independently quantize weights for each output channel in a convolution/linear layer, which can lead to higher accuracy with almost the same speed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# the three lines that perform post-training static quantization on the pre-trained model myModel\n",
    "# set quantization config for server (x86) deployment\n",
    "myModel.qconfig = torch.quantization.get_default_config('fbgemm')\n",
    "\n",
    "# insert observers\n",
    "torch.quantization.prepare(myModel, inplace=True)\n",
    "# Calibrate the model and collect statistics\n",
    "\n",
    "# convert to quantized version\n",
    "torch.quantization.convert(myModel, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* Examples (project ideas!): <a href=\"https://pytorch.org/tutorials/advanced/static_quantization_tutorial.html\">Static quantization with eager execution</a>, <a href=\"https://pytorch.org/tutorials/intermediate/quantized_transfer_learning_tutorial.html\">Quantized transfer learning</a>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Quantization Aware Training (QAT)\n",
    "---\n",
    "* The quantization method that typically results in highest accuracy of the three methods.\n",
    "* With QAT, all weights and activations are “fake quantized” during both the forward and backward passes of training: that is, **float values are rounded to mimic `int8` values, but all computations are still done with floating point numbers**.\n",
    "* Thus, all the weight adjustments during training are made while “aware” of the fact that the model will ultimately be quantized; after quantizing, therefore, this method usually yields higher accuracy than the other two methods.\n",
    "* In PyTorch: `torch.quantization.prepare_qat` inserts fake quantization modules to model quantization and `torch.quantization.convert` actually quantizes the model once training is complete."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\"./assets/tut_compress_qat.png\" style=\"height:350px\">\n",
    "\n",
    "<a href=\"https://towardsdatascience.com/inside-quantization-aware-training-4f91c8837ead\">Image Source</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# specify quantization config for QAT\n",
    "qat_model.qconfig = torch.quantization.get_default_qat_qconfig('fbgemm')\n",
    "\n",
    "# prepare QAT\n",
    "torch.quantization.prepare_qat(qat_model, inplace=True)\n",
    "\n",
    "# convert to quantized version, removing dropout, to check for accuracy on each epoch\n",
    "quantized_model = torch.quantization.convert(qat_model.eval(), inplace=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* Example (project ideas): <a href=\"https://pytorch.org/tutorials/advanced/static_quantization_tutorial.html\">Static quantization with eager execution</a>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Important Notes for Quantization in PyTorch\n",
    "---\n",
    "* Quantization support is restricted to a subset of available operators, depending on the method being used, for a list of supported operators, see the <a href=\"https://pytorch.org/docs/stable/quantization.html\">documentation</a>.\n",
    "* The set of available operators and the quantization numerics also depend on the backend being used to run quantized models. \n",
    "* Currently quantized operators are supported only for **CPU inference** in the following backends: x86 and ARM.\n",
    "* QAT is typically only used in CNN models when post training static or dynamic quantization doesn’t yield sufficient accuracy. This can occur with models that are highly optimized to achieve small size."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### <img src=\"https://img.icons8.com/dusk/64/000000/untested.png\" style=\"height:50px;display:inline\"> Which Quantization Method to Use?\n",
    "---\n",
    "\n",
    "|Model Type| Preferred Scheme| Why|\n",
    "|:----------|:-----------------|:----|\n",
    "|LSTM/RNN|Dynamic Quantization| Throughput dominated by compute/memory bandwidth for **weights**|\n",
    "|BERT/Transformer|Dynamic Quantization| Throughput dominated by compute/memory bandwidth for **weights**|\n",
    "|CNN| Static Quantization| Throughput limited by memory bandwidth for **activations**|\n",
    "|CNN| Quantization Aware Training| In the case where accuracy can't be achieved with static quantization|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Performance Results\n",
    "---\n",
    "    \n",
    "|Model Type| Float Latency (ms)| Quantized Latency (ms)| Inference Performance Gain| Accuracy| Device|\n",
    "|:----------|:-----------------|:------------------------|:--------------------------|:---------| :-------|\n",
    "|BERT|581|\t313| 1.8x| F1 score: 0.902 $\\to$ 0.895 (dynamic quantization) |Xeon-D2191 (1.6GHz)|\n",
    "|Resnet-50|214|\t103| 2x| Top 1 Acc: 76.1 $\\to$ 75.9 (static post-training quantization) |Xeon-D2191 (1.6GHz)|\n",
    "|Mobilenet-v2|97| 17| 5.7x|Top 1 Acc: 71.9 $\\to$ 71.6 (QAT) |Samsung S9|\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## <img src=\"https://img.icons8.com/doodle/96/000000/tree--v1.png\" style=\"height:50px;display:inline\"> Post Train-time Efficiency - Pruning\n",
    "---\n",
    "* Neural network pruning is the process of sparsifying neural networks from pre-trained dense neural networks, and is very similar to the pruning process of decision trees.\n",
    "    * Instead of creating smaller trees, we create smaller computation graphs post-training.\n",
    "* Pruning is used to investigate the differences in learning dynamics between over-parametrized and under-parametrized networks, to study the role of *lucky* sparse subnetworks and initializations (“lottery tickets”) as a neural architecture search technique."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\"./assets/tut_compress_pruning.png\" style=\"height:350px\">\n",
    "\n",
    "<a href=\"https://towardsdatascience.com/how-to-compress-a-neural-network-427e8dddcc34\">Image Source</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### <img src=\"https://img.icons8.com/office/80/000000/gold-bars.png\" style=\"height:50px;display:inline\"> The Lottery Ticket Hypothesis\n",
    "---\n",
    "The <a href=\"https://arxiv.org/abs/1803.03635\">\"Lottery Ticket Hypothesis\"</a> (Jonathan Frankle and Michael Carbin, 2008):\n",
    "\n",
    "> A randomly-initialized, dense neural network contains a subnetwork that is initialized such that — when trained in isolation — it can match the test accuracy of the original network after training for at most the same number of iterations.\n",
    "\n",
    "* Such subnetworks are called *winning lottery tickets*.\n",
    "* If this hypothesis is true, and such subnetworks can be found, training could be done much faster and cheaper, since a single iteration step would take less computation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Iterative Pruning Algorithm** to find *winning lottery tickets*:\n",
    "\n",
    "1. Randomly initialize the network and store the initial weights for later reference.\n",
    "2. Train the network for a given number of steps.\n",
    "3. Remove a percentage of the weights (prune) with the **lowest magnitude**.\n",
    "4. Restore the remaining weights to the value that was given during the first initialization.\n",
    "5. Go to **Step 2** and iterate the pruning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### <img src=\"https://img.icons8.com/cotton/64/000000/olympic-torch.png\" style=\"height:50px;display:inline\"> Pruning in PyTorch\n",
    "---\n",
    "* We will follow an <a href=\"https://pytorch.org/tutorials/intermediate/pruning_tutorial.html\">example</a> by <a href=\"Michela Paganini\">Michela Paganini</a>.\n",
    "* We will use PyTorch pruning name-space - `torch.nn.utils.prune`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# our model for the demonstartion\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class LeNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LeNet, self).__init__()\n",
    "        # 1 input image channel, 6 output channels, 3x3 square conv kernel\n",
    "        self.conv1 = nn.Conv2d(1, 6, 3)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 3)\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)  # 5x5 image dimension\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))\n",
    "        x = F.max_pool2d(F.relu(self.conv2(x)), 2)\n",
    "        x = x.view(-1, int(x.nelement() / x.shape[0]))\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "model = LeNet().to(device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "To prune a module (in this example, the `conv1` layer of the LeNet architecture):\n",
    "1. Select a pruning technique among those available in `torch.nn.utils.prune` (or implement your own by subclassing `BasePruningMethod`).\n",
    "2. Specify the module and the name of the parameter to prune within that module. \n",
    "3. Finally, using the adequate keyword arguments required by the selected pruning technique, specify the pruning parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* In this example, we will prune **at random** 30% of the connections in the parameter named `weight` in the `conv1` layer.\n",
    "* The module is passed as the first argument to the function.\n",
    "    * `name` identifies the parameter within that module using its string identifier.\n",
    "    * `amount` indicates either the percentage of connections to prune (if it is a float between 0. and 1.), or the absolute number of connections to prune (if it is a non-negative integer).\n",
    "    * `prune.random_unstructured(module, name=\"weight\", amount=0.3)`\n",
    "* Pruning acts by removing `weight` from the parameters and replacing it with a new parameter called `weight_orig` (i.e. appending \"\\_orig\" to the initial parameter name). `weight_orig` stores the unpruned version of the tensor. \n",
    "    * The `bias` was not pruned, so it will remain intact."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before pruning:\n",
      "[('weight', Parameter containing:\n",
      "tensor([[[[-0.0020,  0.0248,  0.2009],\n",
      "          [ 0.2769,  0.1085, -0.1986],\n",
      "          [ 0.3300,  0.3240,  0.2869]]],\n",
      "\n",
      "\n",
      "        [[[ 0.3019,  0.0024, -0.0397],\n",
      "          [ 0.0089,  0.1513, -0.2677],\n",
      "          [-0.3236, -0.2904, -0.0882]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1837,  0.1874,  0.1020],\n",
      "          [-0.0874, -0.2104, -0.2653],\n",
      "          [ 0.1155, -0.0086,  0.2760]]],\n",
      "\n",
      "\n",
      "        [[[ 0.2929,  0.0033,  0.1322],\n",
      "          [-0.0203,  0.1317, -0.0450],\n",
      "          [-0.2536, -0.0920, -0.0615]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0360,  0.0032, -0.0793],\n",
      "          [ 0.2970, -0.0857, -0.1989],\n",
      "          [-0.0124, -0.3124, -0.1621]]],\n",
      "\n",
      "\n",
      "        [[[-0.0620,  0.2461,  0.2737],\n",
      "          [ 0.0753,  0.2653,  0.3236],\n",
      "          [ 0.0951,  0.3321,  0.0862]]]], device='cuda:0', requires_grad=True)), ('bias', Parameter containing:\n",
      "tensor([ 0.1776, -0.1092, -0.1042,  0.2878, -0.2541,  0.2821], device='cuda:0',\n",
      "       requires_grad=True))]\n",
      "after pruning:\n",
      "[('bias', Parameter containing:\n",
      "tensor([ 0.1776, -0.1092, -0.1042,  0.2878, -0.2541,  0.2821], device='cuda:0',\n",
      "       requires_grad=True)), ('weight_orig', Parameter containing:\n",
      "tensor([[[[-0.0020,  0.0248,  0.2009],\n",
      "          [ 0.2769,  0.1085, -0.1986],\n",
      "          [ 0.3300,  0.3240,  0.2869]]],\n",
      "\n",
      "\n",
      "        [[[ 0.3019,  0.0024, -0.0397],\n",
      "          [ 0.0089,  0.1513, -0.2677],\n",
      "          [-0.3236, -0.2904, -0.0882]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1837,  0.1874,  0.1020],\n",
      "          [-0.0874, -0.2104, -0.2653],\n",
      "          [ 0.1155, -0.0086,  0.2760]]],\n",
      "\n",
      "\n",
      "        [[[ 0.2929,  0.0033,  0.1322],\n",
      "          [-0.0203,  0.1317, -0.0450],\n",
      "          [-0.2536, -0.0920, -0.0615]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0360,  0.0032, -0.0793],\n",
      "          [ 0.2970, -0.0857, -0.1989],\n",
      "          [-0.0124, -0.3124, -0.1621]]],\n",
      "\n",
      "\n",
      "        [[[-0.0620,  0.2461,  0.2737],\n",
      "          [ 0.0753,  0.2653,  0.3236],\n",
      "          [ 0.0951,  0.3321,  0.0862]]]], device='cuda:0', requires_grad=True))]\n"
     ]
    }
   ],
   "source": [
    "module = model.conv1\n",
    "print(\"before pruning:\")\n",
    "print(list(module.named_parameters()))\n",
    "prune.random_unstructured(module, name=\"weight\", amount=0.3)\n",
    "print(\"after pruning:\")\n",
    "print(list(module.named_parameters()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* The pruning mask generated by the pruning technique selected above is saved as a **module buffer** named `weight_mask` (i.e. appending \"\\_mask\" to the initial parameter name)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('bias_mask', tensor([0., 1., 1., 0., 1., 0.], device='cuda:0')), ('weight_mask', tensor([[[[1., 1., 1.],\n",
      "          [0., 1., 1.],\n",
      "          [1., 1., 1.]]],\n",
      "\n",
      "\n",
      "        [[[1., 0., 0.],\n",
      "          [0., 0., 1.],\n",
      "          [1., 1., 1.]]],\n",
      "\n",
      "\n",
      "        [[[1., 1., 1.],\n",
      "          [1., 1., 1.],\n",
      "          [1., 1., 1.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.],\n",
      "          [1., 1., 1.],\n",
      "          [0., 1., 1.]]],\n",
      "\n",
      "\n",
      "        [[[1., 1., 0.],\n",
      "          [1., 0., 1.],\n",
      "          [0., 1., 1.]]],\n",
      "\n",
      "\n",
      "        [[[1., 1., 1.],\n",
      "          [1., 0., 0.],\n",
      "          [0., 0., 1.]]]], device='cuda:0'))]\n"
     ]
    }
   ],
   "source": [
    "print(list(module.named_buffers()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* For the forward pass to work *without modification*, the `weight` attribute needs to exist. \n",
    "* The pruning techniques implemented in `torch.nn.utils.prune` compute the pruned version of the weight (by combining the mask with the original parameter) and store them in the attribute `weight`. \n",
    "* Note, **this is no longer a parameter of the module**, it is now simply an attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[-0.2732,  0.0000,  0.2346],\n",
      "          [-0.0000, -0.1691, -0.0000],\n",
      "          [-0.0000, -0.0000,  0.0000]]],\n",
      "\n",
      "\n",
      "        [[[-0.0073,  0.0000, -0.0000],\n",
      "          [-0.0000,  0.0000,  0.2844],\n",
      "          [-0.2542, -0.0446,  0.2390]]],\n",
      "\n",
      "\n",
      "        [[[-0.1039,  0.1222,  0.0302],\n",
      "          [-0.0933,  0.2147, -0.1062],\n",
      "          [-0.2784,  0.1784, -0.1429]]],\n",
      "\n",
      "\n",
      "        [[[-0.0000,  0.0000, -0.0000],\n",
      "          [ 0.0000, -0.0000,  0.0071],\n",
      "          [-0.0000,  0.0930, -0.3321]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0193,  0.0072,  0.0000],\n",
      "          [-0.0000, -0.0000,  0.2329],\n",
      "          [ 0.0000,  0.1394,  0.0000]]],\n",
      "\n",
      "\n",
      "        [[[-0.1514,  0.2017,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000],\n",
      "          [-0.0000, -0.0000, -0.0072]]]], device='cuda:0',\n",
      "       grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(module.weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* Finally, pruning is applied **prior to each forward pass** using PyTorch’s `forward_pre_hooks`. \n",
    "* Specifically, when the `module` is pruned, as we have done here, it will acquire a `forward_pre_hook` for each parameter associated with it that gets pruned. \n",
    "* In this case, since we have so far only pruned the original parameter named `weight`, only one hook will be present."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([(1, <torch.nn.utils.prune.L1Unstructured object at 0x000001F4094BD978>), (2, <torch.nn.utils.prune.RandomUnstructured object at 0x000001F4094BDF28>)])\n"
     ]
    }
   ],
   "source": [
    "print(module._forward_pre_hooks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* We can now prune the `bias` too, to see how the parameters, buffers, hooks, and attributes of the module change. \n",
    "* Just for the sake of trying out another pruning technique, here we prune the **3 smallest entries** in the `bias` by **L1 norm**, as implemented in the `l1_unstructured` pruning function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "named parameters:\n",
      "[('bias_orig', Parameter containing:\n",
      "tensor([ 0.1057,  0.1858, -0.2802,  0.0499, -0.2165,  0.0779], device='cuda:0',\n",
      "       requires_grad=True)), ('weight_orig', Parameter containing:\n",
      "tensor([[[[-0.2732,  0.0000,  0.2346],\n",
      "          [-0.1208, -0.1691, -0.0000],\n",
      "          [-0.0000, -0.0000,  0.0000]]],\n",
      "\n",
      "\n",
      "        [[[-0.0073,  0.1105, -0.0000],\n",
      "          [-0.0686,  0.0000,  0.2844],\n",
      "          [-0.2542, -0.0446,  0.2390]]],\n",
      "\n",
      "\n",
      "        [[[-0.1039,  0.1222,  0.0302],\n",
      "          [-0.0933,  0.2147, -0.1062],\n",
      "          [-0.2784,  0.1784, -0.1429]]],\n",
      "\n",
      "\n",
      "        [[[-0.2053,  0.1681, -0.0000],\n",
      "          [ 0.0000, -0.0000,  0.0071],\n",
      "          [-0.0623,  0.0930, -0.3321]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0193,  0.0072,  0.2102],\n",
      "          [-0.0000, -0.0172,  0.2329],\n",
      "          [ 0.1754,  0.1394,  0.0000]]],\n",
      "\n",
      "\n",
      "        [[[-0.1514,  0.2017,  0.0000],\n",
      "          [ 0.0000,  0.2775,  0.0000],\n",
      "          [-0.0000, -0.3004, -0.0072]]]], device='cuda:0', requires_grad=True))]\n",
      "named buffers:\n",
      "[('bias_mask', tensor([0., 0., 0., 0., 0., 0.], device='cuda:0')), ('weight_mask', tensor([[[[1., 1., 1.],\n",
      "          [0., 1., 1.],\n",
      "          [1., 1., 1.]]],\n",
      "\n",
      "\n",
      "        [[[1., 0., 0.],\n",
      "          [0., 0., 1.],\n",
      "          [1., 1., 1.]]],\n",
      "\n",
      "\n",
      "        [[[1., 1., 1.],\n",
      "          [1., 1., 1.],\n",
      "          [1., 1., 1.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.],\n",
      "          [1., 1., 1.],\n",
      "          [0., 1., 1.]]],\n",
      "\n",
      "\n",
      "        [[[1., 1., 0.],\n",
      "          [1., 0., 1.],\n",
      "          [0., 1., 1.]]],\n",
      "\n",
      "\n",
      "        [[[1., 1., 1.],\n",
      "          [1., 0., 0.],\n",
      "          [0., 0., 1.]]]], device='cuda:0'))]\n",
      "module.bias:\n",
      "tensor([0., 0., -0., 0., -0., 0.], device='cuda:0', grad_fn=<MulBackward0>)\n",
      "forward pre hooks:\n",
      "OrderedDict([(2, <torch.nn.utils.prune.RandomUnstructured object at 0x000001F4094BDF28>), (3, <torch.nn.utils.prune.PruningContainer object at 0x000001F4094E7438>)])\n"
     ]
    }
   ],
   "source": [
    "prune.l1_unstructured(module, name=\"bias\", amount=3)\n",
    "print('named parameters:')\n",
    "print(list(module.named_parameters()))  # notice the bias_orig\n",
    "print('named buffers:')\n",
    "print(list(module.named_buffers()))  # notice the bias_mask\n",
    "print('module.bias:')\n",
    "print(module.bias)\n",
    "print('forward pre hooks:')\n",
    "print(module._forward_pre_hooks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* You can now try different methods to prune your trained model.\n",
    "* For more examples (project ideas!): <a href=\"https://pytorch.org/tutorials/intermediate/pruning_tutorial.html#iterative-pruning\">Iterative Pruning</a>,  <a href=\"https://pytorch.org/tutorials/intermediate/pruning_tutorial.html#pruning-multiple-parameters-in-a-model\">Pruning Multiple Parameters in a Model</a>, <a href=\"https://pytorch.org/tutorials/intermediate/pruning_tutorial.html#global-pruning\">Global Pruning (pruning all weights at once)</a>, <a href=\"https://pytorch.org/tutorials/intermediate/pruning_tutorial.html#extending-torch-nn-utils-prune-with-custom-pruning-functions\">Custom Pruning Functions</a>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* Making the pruning **permanent** by removing pruning re-parametrization.\n",
    "    * This means removing the re-parametrization in terms of `weight_orig` and `weight_mask`, and remove the `forward_pre_hook`.\n",
    "* We can use the `remove` functionality from `torch.nn.utils.prune`. \n",
    "    * Note that this doesn’t undo the pruning, as if it never happened. It simply makes it permanent, instead, by reassigning the parameter weight to the model parameters, in its pruned version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('bias_orig', Parameter containing:\n",
      "tensor([ 0.1057,  0.1858, -0.2802,  0.0499, -0.2165,  0.0779], device='cuda:0',\n",
      "       requires_grad=True)), ('weight', Parameter containing:\n",
      "tensor([[[[-0.2732,  0.0000,  0.2346],\n",
      "          [-0.0000, -0.1691, -0.0000],\n",
      "          [-0.0000, -0.0000,  0.0000]]],\n",
      "\n",
      "\n",
      "        [[[-0.0073,  0.0000, -0.0000],\n",
      "          [-0.0000,  0.0000,  0.2844],\n",
      "          [-0.2542, -0.0446,  0.2390]]],\n",
      "\n",
      "\n",
      "        [[[-0.1039,  0.1222,  0.0302],\n",
      "          [-0.0933,  0.2147, -0.1062],\n",
      "          [-0.2784,  0.1784, -0.1429]]],\n",
      "\n",
      "\n",
      "        [[[-0.0000,  0.0000, -0.0000],\n",
      "          [ 0.0000, -0.0000,  0.0071],\n",
      "          [-0.0000,  0.0930, -0.3321]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0193,  0.0072,  0.0000],\n",
      "          [-0.0000, -0.0000,  0.2329],\n",
      "          [ 0.0000,  0.1394,  0.0000]]],\n",
      "\n",
      "\n",
      "        [[[-0.1514,  0.2017,  0.0000],\n",
      "          [ 0.0000,  0.0000,  0.0000],\n",
      "          [-0.0000, -0.0000, -0.0072]]]], device='cuda:0', requires_grad=True))]\n",
      "[('bias_mask', tensor([0., 0., 0., 0., 0., 0.], device='cuda:0'))]\n"
     ]
    }
   ],
   "source": [
    "prune.remove(module, 'weight')\n",
    "print(list(module.named_parameters()))  # only bias_orig remains\n",
    "print(list(module.named_buffers()))  # only bias_mask remains"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### <img src=\"https://img.icons8.com/bubbles/50/000000/video-playlist.png\" style=\"height:50px;display:inline\"> Recommended Videos\n",
    "---\n",
    "#### <img src=\"https://img.icons8.com/cute-clipart/64/000000/warning-shield.png\" style=\"height:30px;display:inline\"> Warning!\n",
    "* These videos do not replace the lectures and tutorials.\n",
    "* Please use these to get a better understanding of the material, and not as an alternative to the written material.\n",
    "\n",
    "#### Video By Subject\n",
    "\n",
    "* Autimatic Mixed Precision (AMP) - <a href=\"https://www.youtube.com/watch?v=b5dAmcBKxHg\"> NVIDIA - Automatic Mixed Precision Training in PyTorch </a>\n",
    "* Quantization - <a href=\"https://www.youtube.com/watch?v=c3MT2qV5f9w\">Deep Dive on PyTorch Quantization - Chris Gottbrath</a>\n",
    "* Pruning - <a href=\"https://www.youtube.com/watch?v=f86hkOGoX54\">Neural Network Pruning for Compression and Understanding - Facebook AI Research - Dr. Michela Paganini</a>\n",
    "    * Pruning - <a href=\"https://www.youtube.com/watch?v=TaOwEa3m5dw\">PyTorch Pruning - How it's Made by Michela Paganini</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## <img src=\"https://img.icons8.com/dusk/64/000000/prize.png\" style=\"height:50px;display:inline\"> Credits\n",
    "---\n",
    "* Icons made by <a href=\"https://www.flaticon.com/authors/becris\" title=\"Becris\">Becris</a> from <a href=\"https://www.flaticon.com/\" title=\"Flaticon\">www.flaticon.com</a>\n",
    "* Icons from <a href=\"https://icons8.com/\">Icons8.com</a> - https://icons8.com\n",
    "* NVIDIA - <a href=\"https://developer.nvidia.com/automatic-mixed-precision\">Mixed Precision</a>\n",
    "* <a href=\"https://pytorch.org/blog/introduction-to-quantization-on-pytorch/\">Introduction to Quantization in PyTorch</a> - by Raghuraman Krishnamoorthi, James Reed, Min Ni, Chris Gottbrath, and Seth Weidman.\n",
    "* Tivadar Danka - <a href=\"https://towardsdatascience.com/how-to-compress-a-neural-network-427e8dddcc34\">How to Compress a Neural Network</a>\n",
    "* <a href=\"Michela Paganini\">Michela Paganini</a> - <a href=\"https://pytorch.org/tutorials/intermediate/pruning_tutorial.html\">Pruning Tutorial</a>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
